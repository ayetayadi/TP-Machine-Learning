# üìò Travaux Pratiques de Machine Learning  
**ISAMM ‚Äì Universit√© de la Manouba**  
**Encadr√© par : Dr. Mohamed Ridha Amamou**  
**R√©alis√© par : Ayet Ayadi, Chaima Hermi ‚Äì 3INLOG1**  
**Ann√©e universitaire : 2025‚Äì2026**

---

## 1. Objectif global du module
Ce projet regroupe l‚Äôensemble des **travaux pratiques du module de Machine Learning**, permettant de ma√Ætriser les algorithmes fondamentaux d‚Äôapprentissage supervis√© et non supervis√©, depuis la r√©gression lin√©aire jusqu‚Äô√† la d√©tection d‚Äôanomalies.

Chaque TP met l‚Äôaccent sur :
- la compr√©hension th√©orique des mod√®les ;
- leur impl√©mentation en Python (avec `NumPy`, `Pandas`, `Matplotlib`, `scikit-learn`) ;
- l‚Äôinterpr√©tation des r√©sultats et la validation des performances.

---

## 2. Contenu des TPs

### üß© TP1 ‚Äî R√©gression Lin√©aire et Descente de Gradient
**Objectif :**  
Impl√©menter et analyser la descente de gradient pour ajuster les param√®tres d‚Äôun mod√®le de r√©gression lin√©aire.

**Points cl√©s :**
- Impl√©mentation du gradient descendant avec normalisation des donn√©es.  
- √âtude de l‚Äôeffet des hyperparam√®tres (`Œ±`, nombre d‚Äôit√©rations).  
- Correction d‚Äôune anomalie de pr√©traitement (normalisation manquante).  
- Extension vers un **mod√®le quadratique** pour capturer les non-lin√©arit√©s.  

**R√©sultats :**
- R√©duction de la MAE apr√®s normalisation et bonne initialisation.  
- Le mod√®le quadratique pr√©sente une meilleure pr√©cision que le mod√®le lin√©aire.

**Remarques :**
- La normalisation de toutes les variables est indispensable.  
- Un bon choix du pas d‚Äôapprentissage am√©liore la convergence.

---

### üîç TP2 ‚Äî K-Nearest Neighbors (KNN) et R√©gression Lin√©aire
**Objectif :**  
Appliquer des m√©thodes supervis√©es pour la classification et la r√©gression.

**M√©thodes :**
- Impl√©mentation de `KNeighborsClassifier` et `KNeighborsRegressor` de `scikit-learn`.  
- Comparaison entre distances **Euclidienne** et **Manhattan**.  
- Application sur les datasets *Forest CoverType* (classification) et *Diabetes* (r√©gression).  
- D√©veloppement d‚Äôun mod√®le de r√©gression lin√©aire pour pr√©dire le prix d‚Äôappartements.

**R√©sultats :**
- Pr√©cision stable pour KNN, l√©g√®re am√©lioration avec distance Manhattan.  
- R√©gression lin√©aire convergente avec un co√ªt final minimal (~1.2√ó10‚Åª‚Å¥).  
- Le mod√®le final :  
  \[
  \hat{y} = 1000 \times (0.1069 + 0.1301 \frac{Surface}{500} - 0.1546 \frac{Etage}{10} + 0.2517 \frac{Nb\_pieces}{10})
  \]

**Remarques :**
- Le choix de `k` et du type de distance influence fortement la performance.  
- La normalisation garantit une meilleure stabilit√© des r√©sultats.

---

### üåÄ TP3.1 ‚Äî K-Means : Impl√©mentation et √âvaluation
**Objectif :**  
Mettre en ≈ìuvre l‚Äôalgorithme de groupement K-Means pour comprendre la logique de regroupement et la convergence des centro√Ødes.

**√âtapes :**
- Impl√©mentation manuelle du K-Means.  
- Comparaison avec l‚Äôimpl√©mentation `KMeans` de `scikit-learn`.  
- √âvaluation des clusters selon la compacit√© (W) et la s√©paration (B).  
- Comparaison entre distances Euclidienne et Manhattan.

**R√©sultats :**
| Distance | W (Intra) | B (Inter) | B/W |
|-----------|------------|-----------|------|
| Euclidienne | 1.731 | 1.402 | 0.810 |
| Manhattan | 1.776 | 1.357 | 0.764 |

**Conclusion :**
- Les clusters Euclidiens sont plus compacts et mieux s√©par√©s.  
- L‚Äôimpl√©mentation manuelle donne des r√©sultats identiques √† `scikit-learn`.

---

### ‚öôÔ∏è TP3.2 ‚Äî D√©tection d‚ÄôAnomalies et Validation par Clustering
**Objectif :**  
D√©tecter des comportements atypiques √† partir d‚Äôun mod√®le statistique et g√©om√©trique combin√©.

**M√©thodes :**
1. **Mod√©lisation statistique :**  
   - Calcul des moyennes, variances et matrice de covariance.  
   - Application du mod√®le de densit√©s partielles (Gaussiennes ind√©pendantes).  
2. **D√©tection d‚Äôanomalies :**  
   - Seuil fixe : Œµ = 0.02  
   - Aucune anomalie d√©tect√©e (toutes les probabilit√©s p(x) > Œµ).  
3. **Validation g√©om√©trique :**  
   - Utilisation du K-Means pour v√©rifier l‚Äôappartenance aux clusters via le rayon intra-cluster.

**R√©sultats :**
- Tous les nouveaux individus sont accept√©s.  
- La base consolid√©e reste statistiquement stable et coh√©rente.

**Remarques :**
- Combiner la probabilit√© et la distance renforce la robustesse du mod√®le.  
- Cette approche hybride est particuli√®rement adapt√©e √† la d√©tection d‚Äôanomalies comportementales.

---

## 3. Outils utilis√©s
- **Langage :** Python 3.11  
- **Biblioth√®ques :**
  - `NumPy`, `Pandas`, `Matplotlib`
  - `scikit-learn`
- **Environnement de travail :**
  - Jupyter Notebook (`.ipynb`)
  - LaTeX (rapport acad√©mique)
  - Visualisation avec Matplotlib et DataFrames

---

## 4. Synth√®se g√©n√©rale
Ces travaux ont permis de :
- Comprendre les m√©canismes fondamentaux des mod√®les d‚Äôapprentissage supervis√© et non supervis√©.  
- Exp√©rimenter le processus complet d‚Äôun projet de Machine Learning : **pr√©paration, apprentissage, √©valuation, interpr√©tation**.  
- D√©velopper des comp√©tences techniques (impl√©mentation, visualisation, analyse) et analytiques (interpr√©tation, choix des hyperparam√®tres).  

Les r√©sultats obtenus d√©montrent une progression claire vers la ma√Ætrise des techniques de mod√©lisation et d‚Äôanalyse de donn√©es, constituant une base solide pour des √©tudes plus avanc√©es en intelligence artificielle.

---

## 5. Structure du projet

