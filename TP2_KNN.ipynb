{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75793013",
   "metadata": {},
   "source": [
    "## TP2 - Exercice 2 - 15/10/2025\n",
    "\n",
    "### Objectif :\n",
    "\n",
    "- Étudier et appliquer des méthodes d’apprentissage supervisé pour prévoir des valeurs ou classer des données :\n",
    "- Comparer les performances du KNN selon k et le type de distance. \n",
    "- Développer une régression linéaire pour prédire le prix des appartements via normalisation et descente de gradient.\n",
    "- Identifier le meilleur modèle et analyser l’influence des caractéristiques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9dd79e",
   "metadata": {},
   "source": [
    "1. Donner la bibliothèque et les fonctions nécessaires sur python qui permet\n",
    "d’appliquer l’algorithme KNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58c76570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliothèque principale\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "# Fonctions pour préparer et évaluer les données\n",
    "from sklearn.preprocessing import StandardScaler   # Pour normaliser les données\n",
    "from sklearn.model_selection import train_test_split  # Pour séparer données train/test\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error  # Pour mesurer la performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574589c",
   "metadata": {},
   "source": [
    "- KNeighborsClassifier → pour classification (prévoir une catégorie comme résultat)\n",
    "- KNeighborsRegressor → pour régression (prévoir une valeur continue comme résultat)\n",
    "\n",
    "- StandardScaler : met les données sur une échelle comparable (très important pour KNN).\n",
    "- train_test_split : divise tes données en ensemble d’entraînement et test.\n",
    "- accuracy_score : mesure la précision pour la classification.\n",
    "- mean_squared_error : mesure l’erreur pour la régression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388cefe2",
   "metadata": {},
   "source": [
    "2. Donner les fonctions qui permettent de calculer les différents types des distances. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60dbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    \"\"\"\n",
    "    Distance Euclidienne entre deux vecteurs a et b\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "\n",
    "def manhattan_distance(a, b):\n",
    "    \"\"\"\n",
    "    Distance de Manhattan entre deux vecteurs a et b\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(a - b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acae8ae",
   "metadata": {},
   "source": [
    "3. Donner quelques bases de données utilisées pour exécuter des benchmark. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def5019",
   "metadata": {},
   "source": [
    "1️⃣ Forest CoverType (Classification multiclasse)\n",
    "\n",
    "- Description : Prédit le type de couverture forestière à partir de caractéristiques topographiques et environnementales (altitude, pente, aspect, type de sol, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d54f905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'échantillons : 581012\n",
      "Nombre de caractéristiques : 54\n",
      "Classes : {1, 2, 3, 4, 5, 6, 7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "# Chargement du dataset\n",
    "forest = fetch_covtype()\n",
    "X_forest, y_forest = forest.data, forest.target\n",
    "\n",
    "# Aperçu\n",
    "print(\"Nombre d'échantillons :\", X_forest.shape[0])\n",
    "print(\"Nombre de caractéristiques :\", X_forest.shape[1])\n",
    "print(\"Classes :\", set(y_forest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48180a92",
   "metadata": {},
   "source": [
    "2️⃣ Diabetes (Régression)\n",
    "\n",
    "- Description : Mesure de progression de la maladie du diabète selon différentes caractéristiques.\n",
    "- Variable cible : Indice de progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137629ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'échantillons : 442\n",
      "Nombre de caractéristiques : 10\n",
      "Valeurs cibles (exemple) : [151.  75. 141. 206. 135.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Chargement du dataset\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes, y_diabetes = diabetes.data, diabetes.target\n",
    "\n",
    "# Aperçu\n",
    "print(\"Nombre d'échantillons :\", X_diabetes.shape[0])\n",
    "print(\"Nombre de caractéristiques :\", X_diabetes.shape[1])\n",
    "print(\"Valeurs cibles (exemple) :\", y_diabetes[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0468de5",
   "metadata": {},
   "source": [
    "4. Utilisant plusieurs types de distances dresser un tableau des résultats selon les deux\n",
    "hyper paramètres considérés dans l’exercice précèdent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b672eb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Forest CoverType Dataset (Classification) =====\n",
      "Distance  euclidean  manhattan\n",
      "k                             \n",
      "1          0.694000   0.694000\n",
      "3          0.692667   0.695333\n",
      "\n",
      "===== Diabetes Dataset (Régression) =====\n",
      "Distance    euclidean    manhattan\n",
      "k                                 \n",
      "1         5966.646617  6320.225564\n",
      "3         3630.467001  3428.751044\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# Préparation des datasets\n",
    "# -------------------------------\n",
    "\n",
    "# Forest CoverType - classification\n",
    "forest = fetch_covtype()\n",
    "X_forest, y_forest = forest.data, forest.target\n",
    "\n",
    "# Sous-échantillonnage pour test rapide (ex : 5000 échantillons)\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(X_forest.shape[0], 5000, replace=False)\n",
    "X_forest_sample = X_forest[indices]\n",
    "y_forest_sample = y_forest[indices]\n",
    "\n",
    "# Split train/test\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "    X_forest_sample, y_forest_sample, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Normalisation\n",
    "scaler_f = StandardScaler()\n",
    "X_train_f = scaler_f.fit_transform(X_train_f)\n",
    "X_test_f = scaler_f.transform(X_test_f)\n",
    "\n",
    "# Diabetes - régression\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes, y_diabetes = diabetes.data, diabetes.target\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    X_diabetes, y_diabetes, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Normalisation Diabetes\n",
    "scaler_d = StandardScaler()\n",
    "X_train_d = scaler_d.fit_transform(X_train_d)\n",
    "X_test_d = scaler_d.transform(X_test_d)\n",
    "\n",
    "# -------------------------------\n",
    "# Définition des hyper-paramètres\n",
    "# -------------------------------\n",
    "k_values = [1, 3]                  \n",
    "distances = ['euclidean', 'manhattan']  \n",
    "\n",
    "# -------------------------------\n",
    "# KNN Classification - Forest\n",
    "# -------------------------------\n",
    "results_forest = []\n",
    "\n",
    "for dist in distances:\n",
    "    for k in k_values:\n",
    "        clf = KNeighborsClassifier(n_neighbors=k, metric=dist)\n",
    "        clf.fit(X_train_f, y_train_f)\n",
    "        y_pred_f = clf.predict(X_test_f)\n",
    "        acc = accuracy_score(y_test_f, y_pred_f)\n",
    "        results_forest.append((k, dist, acc))\n",
    "\n",
    "df_forest = pd.DataFrame(results_forest, columns=['k', 'Distance', 'Accuracy'])\n",
    "df_forest = df_forest.pivot(index='k', columns='Distance', values='Accuracy')\n",
    "print(\"===== Forest CoverType Dataset (Classification) =====\")\n",
    "print(df_forest)\n",
    "\n",
    "# -------------------------------\n",
    "# KNN Régression - Diabetes\n",
    "# -------------------------------\n",
    "results_diabetes = []\n",
    "\n",
    "for dist in distances:\n",
    "    for k in k_values:\n",
    "        reg = KNeighborsRegressor(n_neighbors=k, metric=dist)\n",
    "        reg.fit(X_train_d, y_train_d)\n",
    "        y_pred_d = reg.predict(X_test_d)\n",
    "        mse = mean_squared_error(y_test_d, y_pred_d)\n",
    "        results_diabetes.append((k, dist, mse))\n",
    "\n",
    "df_diabetes = pd.DataFrame(results_diabetes, columns=['k', 'Distance', 'MSE'])\n",
    "df_diabetes = df_diabetes.pivot(index='k', columns='Distance', values='MSE')\n",
    "print(\"\\n===== Diabetes Dataset (Régression) =====\")\n",
    "print(df_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c71ff",
   "metadata": {},
   "source": [
    "- Forest CoverType (Classification) : La précision est d’environ 69 % pour toutes les combinaisons de k et de distance, montrant que KNN est relativement stable sur ce dataset.\n",
    "\n",
    "- Diabetes (Régression) : L’erreur MSE diminue en passant de k=1 à k=3, et la distance Manhattan donne légèrement de meilleurs résultats, indiquant que augmenter k réduit le bruit et l’impact des points atypiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eddbf8",
   "metadata": {},
   "source": [
    "5. Développer un modèle de régression linéaire complet pour la classification des\n",
    "appartements utilisant la normalisation par les valeurs suivantes : 500, 10, 10 et 1000\n",
    "et un pas d’apprentissage 0,08."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "331b2585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DataFrame original ===\n",
      "   Surface  Etage  Nb_pieces  Prix\n",
      "0      135      1          4   240\n",
      "1      105      2          3   165\n",
      "2      125      2          4   210\n",
      "3       90      1          2   150\n",
      "4      105      3          4   190\n",
      "5      125      0          4   240\n",
      "6       85      3          2   140\n",
      "7      140      0          4   250\n",
      "\n",
      "=== DataFrame normalisé ===\n",
      "   Surface_norm  Etage_norm  Nb_pieces_norm  Prix_norm\n",
      "0          0.27         0.1             0.4      0.240\n",
      "1          0.21         0.2             0.3      0.165\n",
      "2          0.25         0.2             0.4      0.210\n",
      "3          0.18         0.1             0.2      0.150\n",
      "4          0.21         0.3             0.4      0.190\n",
      "5          0.25         0.0             0.4      0.240\n",
      "6          0.17         0.3             0.2      0.140\n",
      "7          0.28         0.0             0.4      0.250\n",
      "\n",
      "=== Matrice X avec biais ===\n",
      "[[1.   0.27 0.1  0.4 ]\n",
      " [1.   0.21 0.2  0.3 ]\n",
      " [1.   0.25 0.2  0.4 ]\n",
      " [1.   0.18 0.1  0.2 ]\n",
      " [1.   0.21 0.3  0.4 ]\n",
      " [1.   0.25 0.   0.4 ]\n",
      " [1.   0.17 0.3  0.2 ]\n",
      " [1.   0.28 0.   0.4 ]]\n",
      "\n",
      "=== Paramètres initiaux theta ===\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Coût initial : 0.0204515625\n",
      "Iteration  100 | Coût = 0.00055769 | Theta = [0.16237863 0.04740605 0.00093861 0.07520855]\n",
      "Iteration  200 | Coût = 0.00045527 | Theta = [ 0.15797946  0.0557434  -0.0202663   0.09195422]\n",
      "Iteration  300 | Coût = 0.00037384 | Theta = [ 0.15388251  0.06325534 -0.03886333  0.1071856 ]\n",
      "Iteration  400 | Coût = 0.00030901 | Theta = [ 0.15006805  0.07003318 -0.05515402  0.12105941]\n",
      "Iteration  500 | Coût = 0.00025731 | Theta = [ 0.14651146  0.07615656 -0.06940662  0.13371308]\n",
      "Iteration  600 | Coût = 0.00021600 | Theta = [ 0.14319065  0.08169624 -0.08185883  0.14526923]\n",
      "Iteration  700 | Coût = 0.00018292 | Theta = [ 0.14008568  0.08671517 -0.09272123  0.15583727]\n",
      "Iteration  800 | Coût = 0.00015637 | Theta = [ 0.1371786   0.09126932 -0.10218038  0.1655149 ]\n",
      "Iteration  900 | Coût = 0.00013499 | Theta = [ 0.13445323  0.09540845 -0.11040152  0.17438938]\n",
      "Iteration 1000 | Coût = 0.00011774 | Theta = [ 0.13189493  0.09917684 -0.11753099  0.18253867]\n",
      "\n",
      "=== Paramètres finaux theta ===\n",
      "[[ 0.13189493]\n",
      " [ 0.09917684]\n",
      " [-0.11753099]\n",
      " [ 0.18253867]]\n",
      "\n",
      "Coût final : 0.0001177371876172928\n",
      "\n",
      "=== Prix réels vs prédits ===\n",
      "   Surface  Etage  Nb_pieces  Prix   Prix_Pred\n",
      "0      135      1          4   240  219.935046\n",
      "1      105      2          3   165  183.977469\n",
      "2      125      2          4   210  206.198410\n",
      "3       90      1          2   150  174.501396\n",
      "4      105      3          4   190  190.478237\n",
      "5      125      0          4   240  229.704608\n",
      "6       85      3          2   140  150.003429\n",
      "7      140      0          4   250  232.679913\n"
     ]
    }
   ],
   "source": [
    "  # -------------------------------\n",
    "# 1️⃣ Données des appartements\n",
    "# -------------------------------\n",
    "data = {\n",
    "    'Surface': [135, 105, 125, 90, 105, 125, 85, 140],\n",
    "    'Etage': [1, 2, 2, 1, 3, 0, 3, 0],\n",
    "    'Nb_pieces': [4, 3, 4, 2, 4, 4, 2, 4],\n",
    "    'Prix': [240, 165, 210, 150, 190, 240, 140, 250]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"=== DataFrame original ===\")\n",
    "print(df)\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ Normalisation des caractéristiques\n",
    "# -------------------------------\n",
    "X = df[['Surface', 'Etage', 'Nb_pieces']].values\n",
    "y = df['Prix'].values.reshape(-1, 1)\n",
    "\n",
    "norm_values = np.array([500, 10, 10])\n",
    "X_norm = X / norm_values   # normalisation des caractéristiques\n",
    "y_norm = y / 1000          # normalisation du prix\n",
    "\n",
    "# Création d'un DataFrame pour visualiser les valeurs normalisées\n",
    "df_norm = pd.DataFrame(X_norm, columns=['Surface_norm', 'Etage_norm', 'Nb_pieces_norm'])\n",
    "df_norm['Prix_norm'] = y_norm\n",
    "\n",
    "print(\"\\n=== DataFrame normalisé ===\")\n",
    "print(df_norm)\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Ajouter le biais pour le modèle linéaire\n",
    "# -------------------------------\n",
    "m = X_norm.shape[0]  # nombre d'exemples\n",
    "X_bias = np.hstack([np.ones((m, 1)), X_norm])  # ajout de la colonne 1 pour theta0\n",
    "\n",
    "n = X_bias.shape[1]  # nombre de paramètres\n",
    "theta = np.zeros((n, 1))  # initialisation des paramètres à 0\n",
    "\n",
    "print(\"\\n=== Matrice X avec biais ===\")\n",
    "print(X_bias)\n",
    "print(\"\\n=== Paramètres initiaux theta ===\")\n",
    "print(theta)\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Définir la fonction de coût (Mean Squared Error)\n",
    "# -------------------------------\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/(2*m)) * np.sum((predictions - y)**2)\n",
    "    return cost\n",
    "\n",
    "# Vérification du coût initial\n",
    "initial_cost = compute_cost(X_bias, y_norm, theta)\n",
    "print(\"\\nCoût initial :\", initial_cost)\n",
    "\n",
    "# -------------------------------\n",
    "# -------------------------------\n",
    "# 5️⃣ Descente de gradient avec affichage partiel\n",
    "# -------------------------------\n",
    "alpha = 0.08      # pas d’apprentissage\n",
    "iterations = 1000 # nombre d’itérations\n",
    "cost_history = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    predictions = X_bias.dot(theta)\n",
    "    gradient = (1/m) * X_bias.T.dot(predictions - y_norm)\n",
    "    theta = theta - alpha * gradient\n",
    "    cost = compute_cost(X_bias, y_norm, theta)\n",
    "    cost_history.append(cost)\n",
    "    \n",
    "    # Affichage tous les 100 itérations\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Iteration {i+1:4d} | Coût = {cost:.8f} | Theta = {theta.ravel()}\")\n",
    "\n",
    "print(\"\\n=== Paramètres finaux theta ===\")\n",
    "print(theta)\n",
    "print(\"\\nCoût final :\", cost_history[-1])\n",
    "\n",
    "# -------------------------------\n",
    "# 6️⃣ Prédiction avec le modèle\n",
    "# -------------------------------\n",
    "y_pred_norm = X_bias.dot(theta)\n",
    "y_pred = y_pred_norm * 1000  # remise à l’échelle du prix\n",
    "\n",
    "df['Prix_Pred'] = y_pred\n",
    "print(\"\\n=== Prix réels vs prédits ===\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c1d8a",
   "metadata": {},
   "source": [
    "**Observation :**\n",
    "\n",
    "- Les prix prédits sont proches des prix réels, même avec seulement 8 appartements.\n",
    "- La descente de gradient a convergé rapidement, le coût final est très faible (~0,00012), indiquant un bon ajustement du modèle.\n",
    "- Les coefficients finaux (θ) montrent l’influence de chaque variable :\n",
    "- Surface : plus grande influence positive sur le prix\n",
    "- Étages : influence légèrement négative ou faible\n",
    "- Nombre de pièces : influence positive sur le prix\n",
    "- Le modèle linéaire peut donc approximer correctement les prix pour ce petit dataset après normalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c4b1c",
   "metadata": {},
   "source": [
    "6. Donner le tableau des performances du modèle pour différentes valeurs des hyper\n",
    "parametres. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba46a94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tableau des performances ===\n",
      "Iterations      500       1000      2000\n",
      "Alpha                                   \n",
      "0.01        0.000603  0.000530  0.000412\n",
      "0.05        0.000365  0.000207  0.000088\n",
      "0.08        0.000257  0.000118  0.000050\n",
      "0.10        0.000207  0.000088  0.000042\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Données déjà normalisées\n",
    "X_bias = np.hstack([np.ones((X_norm.shape[0],1)), X_norm])\n",
    "y_norm = y / 1000\n",
    "m = X_bias.shape[0]\n",
    "\n",
    "# Hyperparamètres à tester\n",
    "alphas = [0.01, 0.05, 0.08, 0.1]\n",
    "iterations_list = [500, 1000, 2000]\n",
    "\n",
    "# Tableau pour stocker les résultats\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    for iterations in iterations_list:\n",
    "        theta = np.zeros((X_bias.shape[1],1))\n",
    "        # Descente de gradient\n",
    "        for i in range(iterations):\n",
    "            gradient = (1/m) * X_bias.T.dot(X_bias.dot(theta) - y_norm)\n",
    "            theta = theta - alpha * gradient\n",
    "        # Calcul du coût final\n",
    "        cost_final = (1/(2*m)) * np.sum((X_bias.dot(theta) - y_norm)**2)\n",
    "        results.append((alpha, iterations, cost_final))\n",
    "\n",
    "# Création DataFrame\n",
    "df_perf = pd.DataFrame(results, columns=['Alpha', 'Iterations', 'MSE'])\n",
    "print(\"=== Tableau des performances ===\")\n",
    "print(df_perf.pivot(index='Alpha', columns='Iterations', values='MSE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d47f9e",
   "metadata": {},
   "source": [
    "7. Donner l’expression du meilleur modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e420314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== θ final du meilleur modèle ===\n",
      "        Paramètre    Valeur\n",
      "0  θ0 (intercept)  0.106904\n",
      "1    θ1 (Surface)  0.130104\n",
      "2      θ2 (Etage) -0.154647\n",
      "3  θ3 (Nb_pieces)  0.251722\n"
     ]
    }
   ],
   "source": [
    "# Descente de gradient\n",
    "# -------------------------------\n",
    "alpha = 0.10       # meilleur alpha\n",
    "iterations = 2000  # meilleur nombre d'itérations\n",
    "theta = np.zeros((X_bias.shape[1],1))\n",
    "\n",
    "for i in range(iterations):\n",
    "    gradient = (1/m) * X_bias.T.dot(X_bias.dot(theta) - y_norm)\n",
    "    theta = theta - alpha * gradient\n",
    "\n",
    "# -------------------------------\n",
    "# Affichage du θ final\n",
    "# -------------------------------\n",
    "theta_final = theta.flatten()\n",
    "param_names = ['θ0 (intercept)', 'θ1 (Surface)', 'θ2 (Etage)', 'θ3 (Nb_pieces)']\n",
    "\n",
    "df_theta = pd.DataFrame({'Paramètre': param_names, 'Valeur': theta_final})\n",
    "print(\"=== θ final du meilleur modèle ===\")\n",
    "print(df_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbbdc9",
   "metadata": {},
   "source": [
    "##### 7️⃣ Meilleur modèle de régression linéaire\n",
    "\n",
    "Le meilleur modèle correspond à **α = 0.10** et **2000 itérations**, car il donne le **MSE le plus faible** sur notre dataset.\n",
    "\n",
    "##### 1️⃣ Paramètres finaux (normalisés)\n",
    "| Paramètre | Valeur |\n",
    "|-----------|--------|\n",
    "| θ0 (intercept) | 0.1069 |\n",
    "| θ1 (Surface)   | 0.1301 |\n",
    "| θ2 (Etage)     | -0.1546 |\n",
    "| θ3 (Nb_pieces) | 0.2517 |\n",
    "\n",
    "##### 2️⃣ Équation du modèle (normalisée)\n",
    "\n",
    "$$\n",
    "\\hat{y}_{norm} = 0.1069 + 0.1301 \\cdot X_1^{norm} - 0.1546 \\cdot X_2^{norm} + 0.2517 \\cdot X_3^{norm}\n",
    "$$\n",
    "\n",
    "avec :  \n",
    "\n",
    "$$\n",
    "X_1^{norm} = \\frac{\\text{Surface}}{500}, \\quad \n",
    "X_2^{norm} = \\frac{\\text{Etage}}{10}, \\quad \n",
    "X_3^{norm} = \\frac{\\text{Nb\\_pieces}}{10}\n",
    "$$  \n",
    "\n",
    "##### 3️⃣ Équation du modèle en prix réel\n",
    "\n",
    "$$\n",
    "\\hat{y} = 1000 \\cdot \\Big(0.1069 + 0.1301 \\frac{\\text{Surface}}{500} - 0.1546 \\frac{\\text{Etage}}{10} + 0.2517 \\frac{\\text{Nb\\_pieces}}{10}\\Big)\n",
    "$$\n",
    "\n",
    "**Observation :**  \n",
    "- La **Surface** et le **Nombre de pièces** augmentent significativement le prix,  \n",
    "- L’**Étage** a un effet légèrement négatif sur le prix,  \n",
    "- Ce modèle minimise l’erreur sur notre dataset après normalisation et descente de gradient, donnant ainsi les meilleures prédictions pour ces appartements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4f29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
